class Bank_Brain():
    
    def __init__(self, input_shape, output_shape, learning_rate, gamma):
        
        inputs = layers.Input(shape=(input_shape,))
        common = layers.Dense(64, activation="relu")(inputs)
        common_1 = layers.Dense(128, activation="relu")(common)
        action = layers.Dense(output_shape, activation="softmax")(common_1)
        critic = layers.Dense(1)(common_1)        
        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
        self.decision_maker = keras.Model(inputs=inputs, outputs=[action, critic])  
        self.huber_loss = keras.losses.Huber()
        self.gamma = gamma
        self.eps = np.finfo(np.float32).eps.item()
        
    def budget_revision(self, state):
        state = np.array(state)
        #norm = np.linalg.norm(state)
        #state = state / norm
        transformer = Normalizer().fit(state)  # fit does nothing.
        state = transformer.transform(state)
        state = tf.cast(state, tf.float32)
        action, critic = self.decision_maker(state)
        values = np.squeeze(action)
        if random.randint(0,100) < 990:
            values = values * 100
            for_free_out = int(values[1])
            if for_free_out < 0:
                for_free_out = 0
            for_free_in = int(values[3])
            if for_free_in < 0:
                for_free_in = 0
            products = {
                        'outer': {
                            
                                  'cost': int(values[0]),
                                  'for_free': for_free_out,
                                  },
                        'inner': {
                                  'cost': int(values[2]),
                                  'for_free': for_free_in,
                                  },
                        'month_paym': int(values[4]),
                        }
            mark_bud = values[5] * 3000
            if mark_bud < 0:
                mark_bud = 0
            market_budget = mark_bud
        else:
            products = {
                        'outer': {
                                  'cost': random.randint(0,100),
                                  'for_free':random.randint(0,100)
                                  },
                        'inner': {
                                  'cost': random.randint(0,100),
                                  'for_free':random.randint(0,100)
                                  },
                        'month_paym': random.randint(0,10)*100
                        }
            mark_bud = random.randint(10000,100000)
            market_budget = mark_bud
        return (products, market_budget)
    
    def administer_pill(self, pills, data, period = None):
        eps = self.eps
        gamma = self.gamma
        decision_maker = self.decision_maker
        optimizer = self.optimizer
        action_probs_history = []
        critic_value_history = []
        
        with tf.GradientTape() as tape:
            # Backpropagation
            rewards_history = pills
            if period is None:
                count = len(rewards_history)
                data = data.iloc[-count:]
            else:                
                data = data.iloc[-period:]
                rewards_history = rewards_history[-period:]
                count = period          
            discounted_sum = 0
            returns = []
            for r in rewards_history[::-1]:
                discounted_sum = r + gamma * discounted_sum
                returns.insert(0, discounted_sum)
            returns = np.array(returns)
            if len(returns) > 1:
                returns = (returns - np.mean(returns)) / (np.std(returns) + eps)    
                returns = returns.tolist()
            for num in range(count):
                #norm = np.linalg.norm(data)
                state = np.array([data.iloc[num]])
                #state = state/norm
                transformer = Normalizer().fit(state)  # fit does nothing.
                state = transformer.transform(state)
                state = tf.cast(state, tf.float32)
                a,b = decision_maker(state)
                action_probs_history.append(a)
                critic_value_history.append(b)
            history = zip(action_probs_history, critic_value_history, returns)
            actor_losses = []
            critic_losses = []
            
            for log_prob, value, ret in history:
                diff = ret - value
                actor_losses.append(-log_prob * diff) 
                critic_losses.append(self.huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0)))                
            loss_value = sum(actor_losses) + sum(critic_losses)
            grads = tape.gradient(loss_value, decision_maker.trainable_variables)            
            optimizer.apply_gradients(zip(grads, decision_maker.trainable_variables))
